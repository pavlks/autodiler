{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "pd.get_option(\"display.max_columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_tree(source, limit=0, n=0):\n",
    "    \"\"\"\n",
    "    Function prints the structure (a \"tree\") of json code. The depth can be limited by passing \"limit\". n=0 starts a counter from 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    if n == limit and limit:\n",
    "        return\n",
    "    for key in source:\n",
    "        print(\"    \" * n, n+1, \"-\", key)\n",
    "        if isinstance(source[key], dict):\n",
    "            print_tree(source[key], limit, n+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(pages=0):\n",
    "    \"\"\"\n",
    "    Function parses autodiler.me website. As a result we obtain pandas dataframe with all the advertisements.\n",
    "    \"\"\"\n",
    "    \n",
    "    # new line for f-strings\n",
    "    nl = '\\n'\n",
    "    # start stopwatch\n",
    "    started = time.time()\n",
    "    \n",
    "    url = 'https://www.autodiler.me/automobili/all/'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0'}\n",
    "\n",
    "    n = int()\n",
    "    while True:\n",
    "        # time lapsed\n",
    "        lapsed = int(time.time() - started)\n",
    "        # page number\n",
    "        n += 1\n",
    "        print(f\">>>   {lapsed} s   Parsing page number  {n}\", end='   ')\n",
    "        \n",
    "        r = requests.get(url + str(n), headers=headers)\n",
    "        if r.status_code != 200:\n",
    "            print(f'status_code:  {r.status_code}')\n",
    "\n",
    "        # optionally we can save the response to a file\n",
    "#         with open(f'/home/pavlk/Documents/python-projects/autodiler/pages/page-{num}.txt', 'wb') as f:\n",
    "#             f.write(r.content)\n",
    "\n",
    "        # and then work with every file individually\n",
    "#         files_dir = os.path.join(os.getcwd(), 'pages')\n",
    "#         files = os.scandir(files_dir)\n",
    "#         for file in files:\n",
    "#             code = BeautifulSoup(open(os.path.join(files_dir, file), 'rb'), 'html.parser')\n",
    "\n",
    "        # pass response to Beautiful Soup for parsing\n",
    "        code = BeautifulSoup(r.content, 'html.parser')\n",
    "        # find all script tags and take the second one\n",
    "        script_tag = code.find_all('script')[1].string\n",
    "        # strip all the unnecessary information in that tag\n",
    "        json_string = script_tag[\n",
    "            script_tag.find('{'):\n",
    "            script_tag.find('module={')-1\n",
    "            ].strip()\n",
    "        # load parsed json string\n",
    "        src = json.loads(json_string)\n",
    "\n",
    "        # if necessary\n",
    "#         print_tree(src)\n",
    "\n",
    "        data = pd.DataFrame(src['props']['initialState']['ads']['list']['elements'])\n",
    "\n",
    "        # exit the cycle if nothing was obtained\n",
    "        if data.shape[0] == 0 and n == 1:\n",
    "            print(f'data.shape:  {data.shape}')\n",
    "            return None\n",
    "        elif data.shape[0] == 0:\n",
    "            print(f'{nl}{nl}done!')\n",
    "            return ads\n",
    "        \n",
    "        \n",
    "        \n",
    "        # MOVING TO PANDAS AND DATA PROCESSING\n",
    "        \n",
    "        # kick out the first wierd row and column\n",
    "        data.dropna(subset=['code'], inplace=True)\n",
    "        # drop useless columns\n",
    "        data.drop(labels=['banner', 'labelDescription', 'compatibilities'], axis=1, inplace=True)\n",
    "        # look foor empty columns...\n",
    "        empty = data.count().loc[data.count()==0].index\n",
    "        # ...and drop them\n",
    "        data = data.drop(labels=empty, axis=1)\n",
    "        \n",
    "        # get ad image\n",
    "        # if statement in lambda checks whether info exists, i.e. list populated\n",
    "        data['media'] = data['media'].apply(lambda x: x[0]['fd'] if bool(len(x)) else None)\n",
    "        \n",
    "        # function for building publication link\n",
    "        def generate_adlink(row):\n",
    "            return 'https://www.autodiler.me/' + row['categoryTitle'] + '/' + row['seo']\n",
    "        \n",
    "        # create publications links\n",
    "        data['ad_link'] = data.apply(generate_adlink, axis=1)\n",
    "        \n",
    "        # get ad location and location id\n",
    "        data['country'] = data['city'].dropna().apply(lambda x: x['country'])\n",
    "        data['region'] = data['city'].dropna().apply(lambda x: x['region'])\n",
    "        data['city_id'] = data['city'].dropna().apply(lambda x: x['id'])\n",
    "        data['city'] = data['city'].dropna().apply(lambda x: x['name'])\n",
    "        \n",
    "        # get vehicle make and its id\n",
    "        data['brand_id'] = data['brand'].dropna().apply(lambda x: x['id'])\n",
    "        data['brand'] = data['brand'].dropna().apply(lambda x: x['name'])\n",
    "        \n",
    "        # get vehicle model, series and model id\n",
    "        data['product_id'] = data['product'].dropna().apply(lambda x: x['id'])\n",
    "        data['series'] = data['product'].dropna().apply(lambda x: x['series'])\n",
    "        data['model'] = data['product'].dropna().apply(lambda x: x['model'])\n",
    "        \n",
    "        # drop thumbnail column\n",
    "        data = data.drop('thumbnail', axis=1)\n",
    "\n",
    "        # process a set of dates\n",
    "        for date_col in ['createdAt', 'listingExpiration', 'listingActivated', 'activated']:\n",
    "            # !sometimes columns \"listingExpiration\" and \"listingActivated\" are excluded\n",
    "            if date_col in data.columns:\n",
    "                data[date_col] = pd.to_datetime(data[date_col], format='%Y-%m-%dT%H:%M:%S.000Z')\n",
    "\n",
    "        # functions to get fuel type, milage and year\n",
    "        def get_fuel(spec_list):\n",
    "            for spec in spec_list:\n",
    "                if str(spec['title']).lower().startswith('gorivo'):\n",
    "                    return spec['value']\n",
    "\n",
    "        def get_year(spec_list):\n",
    "            for spec in spec_list:\n",
    "                if str(spec['title']).lower().startswith('godi'):\n",
    "                    return spec['value']\n",
    "\n",
    "        def get_km(spec_list):\n",
    "            for spec in spec_list:\n",
    "                if str(spec['title']).lower().startswith('kilometra'):\n",
    "                    return spec['value']\n",
    "                \n",
    "        # apply above metioned functions\n",
    "        data['fuel'] = data['specifications'].dropna().apply(get_fuel)\n",
    "        data['km'] = data['specifications'].dropna().apply(get_km)\n",
    "        data['year'] = data['specifications'].dropna().apply(get_year)\n",
    "        \n",
    "        # appending dataframes\n",
    "        if n == 1:\n",
    "            ads = data.copy()\n",
    "            print(f\"{ads.shape[0]} rows\")\n",
    "        else:\n",
    "            ads = ads.append(data, ignore_index=True)\n",
    "            print(f\"{ads.shape[0]} rows\")\n",
    "            \n",
    "        # break if page limit was set\n",
    "        if n == pages:\n",
    "            print(f'{nl}{nl}{n}  pages were parsed')\n",
    "            return ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>   0 s   Parsing page number  1   36 rows\n",
      ">>>   1 s   Parsing page number  2   72 rows\n",
      ">>>   3 s   Parsing page number  3   108 rows\n",
      ">>>   4 s   Parsing page number  4   144 rows\n",
      ">>>   6 s   Parsing page number  5   180 rows\n",
      ">>>   8 s   Parsing page number  6   216 rows\n",
      ">>>   11 s   Parsing page number  7   252 rows\n",
      ">>>   13 s   Parsing page number  8   288 rows\n",
      ">>>   15 s   Parsing page number  9   324 rows\n",
      ">>>   16 s   Parsing page number  10   360 rows\n",
      ">>>   18 s   Parsing page number  11   396 rows\n",
      ">>>   20 s   Parsing page number  12   432 rows\n",
      ">>>   22 s   Parsing page number  13   468 rows\n",
      ">>>   24 s   Parsing page number  14   504 rows\n",
      ">>>   26 s   Parsing page number  15   540 rows\n",
      ">>>   27 s   Parsing page number  16   576 rows\n",
      ">>>   29 s   Parsing page number  17   612 rows\n",
      ">>>   31 s   Parsing page number  18   648 rows\n",
      ">>>   32 s   Parsing page number  19   684 rows\n",
      ">>>   34 s   Parsing page number  20   720 rows\n",
      ">>>   36 s   Parsing page number  21   756 rows\n",
      ">>>   38 s   Parsing page number  22   792 rows\n",
      ">>>   40 s   Parsing page number  23   828 rows\n",
      ">>>   41 s   Parsing page number  24   864 rows\n",
      ">>>   43 s   Parsing page number  25   900 rows\n",
      ">>>   45 s   Parsing page number  26   936 rows\n",
      ">>>   46 s   Parsing page number  27   972 rows\n",
      ">>>   48 s   Parsing page number  28   1008 rows\n",
      ">>>   49 s   Parsing page number  29   1044 rows\n",
      ">>>   51 s   Parsing page number  30   1080 rows\n",
      ">>>   53 s   Parsing page number  31   1116 rows\n",
      ">>>   54 s   Parsing page number  32   1152 rows\n",
      ">>>   56 s   Parsing page number  33   1188 rows\n",
      ">>>   58 s   Parsing page number  34   1224 rows\n",
      ">>>   60 s   Parsing page number  35   1260 rows\n",
      ">>>   62 s   Parsing page number  36   1296 rows\n",
      ">>>   64 s   Parsing page number  37   1332 rows\n",
      ">>>   66 s   Parsing page number  38   1368 rows\n",
      ">>>   68 s   Parsing page number  39   1404 rows\n",
      ">>>   69 s   Parsing page number  40   1440 rows\n",
      ">>>   71 s   Parsing page number  41   1476 rows\n",
      ">>>   73 s   Parsing page number  42   1512 rows\n",
      ">>>   75 s   Parsing page number  43   1548 rows\n",
      ">>>   76 s   Parsing page number  44   1584 rows\n",
      ">>>   78 s   Parsing page number  45   1620 rows\n",
      ">>>   81 s   Parsing page number  46   1656 rows\n",
      ">>>   82 s   Parsing page number  47   1692 rows\n",
      ">>>   84 s   Parsing page number  48   1728 rows\n",
      ">>>   86 s   Parsing page number  49   1764 rows\n",
      ">>>   88 s   Parsing page number  50   1800 rows\n",
      ">>>   91 s   Parsing page number  51   1836 rows\n",
      ">>>   92 s   Parsing page number  52   1872 rows\n",
      ">>>   94 s   Parsing page number  53   1908 rows\n",
      ">>>   96 s   Parsing page number  54   1944 rows\n",
      ">>>   97 s   Parsing page number  55   1980 rows\n",
      ">>>   99 s   Parsing page number  56   2016 rows\n",
      ">>>   101 s   Parsing page number  57   2052 rows\n",
      ">>>   108 s   Parsing page number  58   2088 rows\n",
      ">>>   109 s   Parsing page number  59   2124 rows\n",
      ">>>   111 s   Parsing page number  60   2160 rows\n",
      ">>>   113 s   Parsing page number  61   2196 rows\n",
      ">>>   114 s   Parsing page number  62   2232 rows\n",
      ">>>   116 s   Parsing page number  63   2268 rows\n",
      ">>>   117 s   Parsing page number  64   2304 rows\n",
      ">>>   119 s   Parsing page number  65   2340 rows\n",
      ">>>   121 s   Parsing page number  66   2376 rows\n",
      ">>>   123 s   Parsing page number  67   2412 rows\n",
      ">>>   125 s   Parsing page number  68   2448 rows\n",
      ">>>   128 s   Parsing page number  69   2484 rows\n",
      ">>>   130 s   Parsing page number  70   2520 rows\n",
      ">>>   132 s   Parsing page number  71   2556 rows\n",
      ">>>   134 s   Parsing page number  72   2592 rows\n",
      ">>>   135 s   Parsing page number  73   2628 rows\n",
      ">>>   137 s   Parsing page number  74   2664 rows\n",
      ">>>   139 s   Parsing page number  75   2700 rows\n",
      ">>>   140 s   Parsing page number  76   2736 rows\n",
      ">>>   142 s   Parsing page number  77   2772 rows\n",
      ">>>   144 s   Parsing page number  78   2808 rows\n",
      ">>>   145 s   Parsing page number  79   2844 rows\n",
      ">>>   147 s   Parsing page number  80   2880 rows\n",
      ">>>   150 s   Parsing page number  81   2916 rows\n",
      ">>>   152 s   Parsing page number  82   2952 rows\n",
      ">>>   154 s   Parsing page number  83   2988 rows\n",
      ">>>   156 s   Parsing page number  84   3024 rows\n",
      ">>>   158 s   Parsing page number  85   3060 rows\n",
      ">>>   160 s   Parsing page number  86   3096 rows\n",
      ">>>   163 s   Parsing page number  87   3132 rows\n",
      ">>>   167 s   Parsing page number  88   3168 rows\n",
      ">>>   169 s   Parsing page number  89   3204 rows\n",
      ">>>   181 s   Parsing page number  90   3240 rows\n",
      ">>>   183 s   Parsing page number  91   3276 rows\n",
      ">>>   188 s   Parsing page number  92   3312 rows\n",
      ">>>   191 s   Parsing page number  93   3348 rows\n",
      ">>>   194 s   Parsing page number  94   3384 rows\n",
      ">>>   196 s   Parsing page number  95   3420 rows\n",
      ">>>   198 s   Parsing page number  96   3456 rows\n",
      ">>>   199 s   Parsing page number  97   3492 rows\n",
      ">>>   201 s   Parsing page number  98   3528 rows\n",
      ">>>   203 s   Parsing page number  99   3564 rows\n",
      ">>>   205 s   Parsing page number  100   3600 rows\n",
      ">>>   207 s   Parsing page number  101   3636 rows\n",
      ">>>   209 s   Parsing page number  102   3672 rows\n",
      ">>>   212 s   Parsing page number  103   3708 rows\n",
      ">>>   214 s   Parsing page number  104   3744 rows\n",
      ">>>   216 s   Parsing page number  105   3780 rows\n",
      ">>>   218 s   Parsing page number  106   3816 rows\n",
      ">>>   219 s   Parsing page number  107   3852 rows\n",
      ">>>   221 s   Parsing page number  108   3888 rows\n",
      ">>>   223 s   Parsing page number  109   3924 rows\n",
      ">>>   224 s   Parsing page number  110   3960 rows\n",
      ">>>   226 s   Parsing page number  111   3996 rows\n",
      ">>>   228 s   Parsing page number  112   4032 rows\n",
      ">>>   230 s   Parsing page number  113   4068 rows\n",
      ">>>   231 s   Parsing page number  114   4104 rows\n",
      ">>>   233 s   Parsing page number  115   4140 rows\n",
      ">>>   235 s   Parsing page number  116   4176 rows\n",
      ">>>   237 s   Parsing page number  117   4212 rows\n",
      ">>>   238 s   Parsing page number  118   4248 rows\n",
      ">>>   240 s   Parsing page number  119   4284 rows\n",
      ">>>   244 s   Parsing page number  120   4320 rows\n",
      ">>>   246 s   Parsing page number  121   4356 rows\n",
      ">>>   248 s   Parsing page number  122   4392 rows\n",
      ">>>   250 s   Parsing page number  123   4428 rows\n",
      ">>>   251 s   Parsing page number  124   4464 rows\n",
      ">>>   253 s   Parsing page number  125   4500 rows\n",
      ">>>   256 s   Parsing page number  126   4536 rows\n",
      ">>>   257 s   Parsing page number  127   4572 rows\n",
      ">>>   259 s   Parsing page number  128   4608 rows\n",
      ">>>   261 s   Parsing page number  129   4644 rows\n",
      ">>>   263 s   Parsing page number  130   4680 rows\n",
      ">>>   265 s   Parsing page number  131   4716 rows\n",
      ">>>   267 s   Parsing page number  132   4752 rows\n",
      ">>>   268 s   Parsing page number  133   4788 rows\n",
      ">>>   270 s   Parsing page number  134   4824 rows\n",
      ">>>   272 s   Parsing page number  135   4860 rows\n",
      ">>>   274 s   Parsing page number  136   4896 rows\n",
      ">>>   276 s   Parsing page number  137   4932 rows\n",
      ">>>   278 s   Parsing page number  138   4968 rows\n",
      ">>>   279 s   Parsing page number  139   5004 rows\n",
      ">>>   281 s   Parsing page number  140   5040 rows\n",
      ">>>   283 s   Parsing page number  141   5076 rows\n",
      ">>>   284 s   Parsing page number  142   5112 rows\n",
      ">>>   286 s   Parsing page number  143   5148 rows\n",
      ">>>   287 s   Parsing page number  144   5184 rows\n",
      ">>>   289 s   Parsing page number  145   5220 rows\n",
      ">>>   291 s   Parsing page number  146   5256 rows\n",
      ">>>   293 s   Parsing page number  147   5292 rows\n",
      ">>>   295 s   Parsing page number  148   5328 rows\n",
      ">>>   297 s   Parsing page number  149   5364 rows\n",
      ">>>   298 s   Parsing page number  150   5400 rows\n",
      ">>>   301 s   Parsing page number  151   5436 rows\n",
      ">>>   303 s   Parsing page number  152   5472 rows\n",
      ">>>   305 s   Parsing page number  153   5508 rows\n",
      ">>>   306 s   Parsing page number  154   5544 rows\n",
      ">>>   308 s   Parsing page number  155   5580 rows\n",
      ">>>   310 s   Parsing page number  156   5616 rows\n",
      ">>>   312 s   Parsing page number  157   5652 rows\n",
      ">>>   314 s   Parsing page number  158   5688 rows\n",
      ">>>   316 s   Parsing page number  159   5724 rows\n",
      ">>>   318 s   Parsing page number  160   5760 rows\n",
      ">>>   319 s   Parsing page number  161   5796 rows\n",
      ">>>   321 s   Parsing page number  162   5832 rows\n",
      ">>>   323 s   Parsing page number  163   5868 rows\n",
      ">>>   325 s   Parsing page number  164   5904 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>   327 s   Parsing page number  165   5940 rows\n",
      ">>>   328 s   Parsing page number  166   5976 rows\n",
      ">>>   331 s   Parsing page number  167   6012 rows\n",
      ">>>   332 s   Parsing page number  168   6048 rows\n",
      ">>>   334 s   Parsing page number  169   6084 rows\n",
      ">>>   337 s   Parsing page number  170   6120 rows\n",
      ">>>   339 s   Parsing page number  171   6156 rows\n",
      ">>>   341 s   Parsing page number  172   6192 rows\n",
      ">>>   342 s   Parsing page number  173   6228 rows\n",
      ">>>   344 s   Parsing page number  174   6264 rows\n",
      ">>>   346 s   Parsing page number  175   6300 rows\n",
      ">>>   347 s   Parsing page number  176   6336 rows\n",
      ">>>   349 s   Parsing page number  177   6372 rows\n",
      ">>>   351 s   Parsing page number  178   6408 rows\n",
      ">>>   353 s   Parsing page number  179   6444 rows\n",
      ">>>   354 s   Parsing page number  180   6480 rows\n",
      ">>>   356 s   Parsing page number  181   6516 rows\n",
      ">>>   358 s   Parsing page number  182   6552 rows\n",
      ">>>   360 s   Parsing page number  183   6588 rows\n",
      ">>>   362 s   Parsing page number  184   6624 rows\n",
      ">>>   365 s   Parsing page number  185   6660 rows\n",
      ">>>   367 s   Parsing page number  186   6696 rows\n",
      ">>>   368 s   Parsing page number  187   6732 rows\n",
      ">>>   370 s   Parsing page number  188   6768 rows\n",
      ">>>   373 s   Parsing page number  189   6804 rows\n",
      ">>>   375 s   Parsing page number  190   6840 rows\n",
      ">>>   376 s   Parsing page number  191   6876 rows\n",
      ">>>   379 s   Parsing page number  192   6912 rows\n",
      ">>>   381 s   Parsing page number  193   6948 rows\n",
      ">>>   383 s   Parsing page number  194   6984 rows\n",
      ">>>   385 s   Parsing page number  195   7020 rows\n",
      ">>>   387 s   Parsing page number  196   7056 rows\n",
      ">>>   389 s   Parsing page number  197   7092 rows\n",
      ">>>   391 s   Parsing page number  198   7128 rows\n",
      ">>>   393 s   Parsing page number  199   7164 rows\n",
      ">>>   394 s   Parsing page number  200   7200 rows\n",
      ">>>   396 s   Parsing page number  201   7236 rows\n",
      ">>>   398 s   Parsing page number  202   7272 rows\n",
      ">>>   400 s   Parsing page number  203   7308 rows\n",
      ">>>   402 s   Parsing page number  204   7344 rows\n",
      ">>>   404 s   Parsing page number  205   7380 rows\n",
      ">>>   406 s   Parsing page number  206   7416 rows\n",
      ">>>   408 s   Parsing page number  207   7452 rows\n",
      ">>>   409 s   Parsing page number  208   7488 rows\n",
      ">>>   412 s   Parsing page number  209   7524 rows\n",
      ">>>   414 s   Parsing page number  210   7560 rows\n",
      ">>>   416 s   Parsing page number  211   7596 rows\n",
      ">>>   418 s   Parsing page number  212   7632 rows\n",
      ">>>   420 s   Parsing page number  213   7668 rows\n",
      ">>>   422 s   Parsing page number  214   7704 rows\n",
      ">>>   424 s   Parsing page number  215   7740 rows\n",
      ">>>   426 s   Parsing page number  216   7776 rows\n",
      ">>>   428 s   Parsing page number  217   7812 rows\n",
      ">>>   429 s   Parsing page number  218   7848 rows\n",
      ">>>   432 s   Parsing page number  219   7884 rows\n",
      ">>>   433 s   Parsing page number  220   7920 rows\n",
      ">>>   435 s   Parsing page number  221   7956 rows\n",
      ">>>   438 s   Parsing page number  222   7992 rows\n",
      ">>>   439 s   Parsing page number  223   8028 rows\n",
      ">>>   441 s   Parsing page number  224   8064 rows\n",
      ">>>   444 s   Parsing page number  225   8100 rows\n",
      ">>>   445 s   Parsing page number  226   8136 rows\n",
      ">>>   447 s   Parsing page number  227   8172 rows\n",
      ">>>   449 s   Parsing page number  228   8208 rows\n",
      ">>>   451 s   Parsing page number  229   8244 rows\n",
      ">>>   452 s   Parsing page number  230   8280 rows\n",
      ">>>   454 s   Parsing page number  231   8316 rows\n",
      ">>>   456 s   Parsing page number  232   8352 rows\n",
      ">>>   458 s   Parsing page number  233   8388 rows\n",
      ">>>   461 s   Parsing page number  234   8424 rows\n",
      ">>>   462 s   Parsing page number  235   8460 rows\n",
      ">>>   464 s   Parsing page number  236   8496 rows\n",
      ">>>   466 s   Parsing page number  237   8532 rows\n",
      ">>>   467 s   Parsing page number  238   8568 rows\n",
      ">>>   469 s   Parsing page number  239   8604 rows\n",
      ">>>   470 s   Parsing page number  240   8640 rows\n",
      ">>>   472 s   Parsing page number  241   8676 rows\n",
      ">>>   474 s   Parsing page number  242   8712 rows\n",
      ">>>   477 s   Parsing page number  243   8748 rows\n",
      ">>>   480 s   Parsing page number  244   8784 rows\n",
      ">>>   482 s   Parsing page number  245   8820 rows\n",
      ">>>   484 s   Parsing page number  246   8856 rows\n",
      ">>>   486 s   Parsing page number  247   8892 rows\n",
      ">>>   487 s   Parsing page number  248   8928 rows\n",
      ">>>   489 s   Parsing page number  249   8964 rows\n",
      ">>>   491 s   Parsing page number  250   9000 rows\n",
      ">>>   493 s   Parsing page number  251   9036 rows\n",
      ">>>   494 s   Parsing page number  252   9072 rows\n",
      ">>>   496 s   Parsing page number  253   9108 rows\n",
      ">>>   497 s   Parsing page number  254   9144 rows\n",
      ">>>   499 s   Parsing page number  255   9180 rows\n",
      ">>>   501 s   Parsing page number  256   9216 rows\n",
      ">>>   502 s   Parsing page number  257   9252 rows\n",
      ">>>   504 s   Parsing page number  258   9288 rows\n",
      ">>>   506 s   Parsing page number  259   9324 rows\n",
      ">>>   509 s   Parsing page number  260   9360 rows\n",
      ">>>   510 s   Parsing page number  261   9396 rows\n",
      ">>>   512 s   Parsing page number  262   9432 rows\n",
      ">>>   514 s   Parsing page number  263   9468 rows\n",
      ">>>   516 s   Parsing page number  264   9504 rows\n",
      ">>>   517 s   Parsing page number  265   9540 rows\n",
      ">>>   519 s   Parsing page number  266   9576 rows\n",
      ">>>   521 s   Parsing page number  267   9612 rows\n",
      ">>>   522 s   Parsing page number  268   9648 rows\n",
      ">>>   524 s   Parsing page number  269   9684 rows\n",
      ">>>   526 s   Parsing page number  270   9720 rows\n",
      ">>>   527 s   Parsing page number  271   9756 rows\n",
      ">>>   529 s   Parsing page number  272   9792 rows\n",
      ">>>   531 s   Parsing page number  273   9828 rows\n",
      ">>>   533 s   Parsing page number  274   9864 rows\n",
      ">>>   534 s   Parsing page number  275   9900 rows\n",
      ">>>   536 s   Parsing page number  276   9936 rows\n",
      ">>>   538 s   Parsing page number  277   9972 rows\n",
      ">>>   539 s   Parsing page number  278   \n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "ads = parsing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUGGING ZONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>   Parsing page number  197\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.autodiler.me/automobili/all/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0'}\n",
    "\n",
    "# set page number\n",
    "n = 197\n",
    "\n",
    "print(f\">>>   Parsing page number  {n}\")\n",
    "r = requests.get(url + str(n), headers=headers)\n",
    "if r.status_code != 200:\n",
    "    print(f'status_code: {r.status_code}   >>>   page number: {num}')\n",
    "\n",
    "# optionally we can save the response to a file\n",
    "#         with open(f'/home/pavlk/Documents/python-projects/autodiler/pages/page-{num}.txt', 'wb') as f:\n",
    "#             f.write(r.content)\n",
    "\n",
    "# and then work with every file individually\n",
    "#         files_dir = os.path.join(os.getcwd(), 'pages')\n",
    "#         files = os.scandir(files_dir)\n",
    "#         for file in files:\n",
    "#             code = BeautifulSoup(open(os.path.join(files_dir, file), 'rb'), 'html.parser')\n",
    "\n",
    "# pass response to Beautiful Soup for parsing\n",
    "code = BeautifulSoup(r.content, 'html.parser')\n",
    "# find all script tags and take the second one\n",
    "script_tag = code.find_all('script')[1].string\n",
    "# strip all the unnecessary information in that tag\n",
    "json_string = script_tag[\n",
    "    script_tag.find('{'):\n",
    "    script_tag.find('module={')-1\n",
    "    ].strip()\n",
    "# load parsed json string\n",
    "src = json.loads(json_string)\n",
    "\n",
    "# if necessary\n",
    "#         print_tree(src)\n",
    "\n",
    "data = pd.DataFrame(src['props']['initialState']['ads']['list']['elements'])\n",
    "\n",
    "# exit the cycle if nothing was obtained\n",
    "if data.shape[0] == 0:\n",
    "    print('::ERROR::Something has changed...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOVING TO PANDAS AND DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kick out the first wierd row and column\n",
    "data.dropna(subset=['code'], inplace=True)\n",
    "# drop useless columns\n",
    "data.drop(labels=['banner', 'labelDescription', 'compatibilities'], axis=1, inplace=True)\n",
    "# look foor empty columns...\n",
    "empty = data.count().loc[data.count()==0].index\n",
    "# ...and drop them\n",
    "data = data.drop(labels=empty, axis=1)\n",
    "\n",
    "# get ad image\n",
    "# if statement in lambda checks whether info exists, i.e. list populated\n",
    "data['media'] = data['media'].apply(lambda x: x[0]['fd'] if bool(len(x)) else None)\n",
    "\n",
    "# function for building publication link\n",
    "def generate_adlink(row):\n",
    "    return 'https://www.autodiler.me/' + row['categoryTitle'] + '/' + row['seo']\n",
    "\n",
    "# create publications links\n",
    "data['ad_link'] = data.apply(generate_adlink, axis=1)\n",
    "\n",
    "# get ad location and location id\n",
    "data['country'] = data['city'].dropna().apply(lambda x: x['country'])\n",
    "data['region'] = data['city'].dropna().apply(lambda x: x['region'])\n",
    "data['city_id'] = data['city'].dropna().apply(lambda x: x['id'])\n",
    "data['city'] = data['city'].dropna().apply(lambda x: x['name'])\n",
    "\n",
    "# get vehicle make and its id\n",
    "data['brand_id'] = data['brand'].dropna().apply(lambda x: x['id'])\n",
    "data['brand'] = data['brand'].dropna().apply(lambda x: x['name'])\n",
    "\n",
    "# get vehicle model, series and model id\n",
    "data['product_id'] = data['product'].dropna().apply(lambda x: x['id'])\n",
    "data['series'] = data['product'].dropna().apply(lambda x: x['series'])\n",
    "data['model'] = data['product'].dropna().apply(lambda x: x['model'])\n",
    "\n",
    "# drop thumbnail column\n",
    "data = data.drop('thumbnail', axis=1)\n",
    "\n",
    "# process a set of dates\n",
    "for date_col in ['createdAt', 'listingExpiration', 'listingActivated', 'activated']:\n",
    "    # !sometimes columns \"listingExpiration\" and \"listingActivated\" are excluded\n",
    "    if date_col in data.columns:\n",
    "        data[date_col] = pd.to_datetime(data[date_col], format='%Y-%m-%dT%H:%M:%S.000Z')\n",
    "\n",
    "# functions to get fuel type, milage and year\n",
    "def get_fuel(spec_list):\n",
    "    for spec in spec_list:\n",
    "        if str(spec['title']).lower().startswith('gorivo'):\n",
    "            return spec['value']\n",
    "\n",
    "def get_year(spec_list):\n",
    "    for spec in spec_list:\n",
    "        if str(spec['title']).lower().startswith('godi'):\n",
    "            return spec['value']\n",
    "\n",
    "def get_km(spec_list):\n",
    "    for spec in spec_list:\n",
    "        if str(spec['title']).lower().startswith('kilometra'):\n",
    "            return spec['value']\n",
    "\n",
    "# apply above metioned functions\n",
    "data['fuel'] = data['specifications'].dropna().apply(get_fuel)\n",
    "data['km'] = data['specifications'].dropna().apply(get_km)\n",
    "data['year'] = data['specifications'].dropna().apply(get_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [{'measure': 'km', 'weightCatalog': 1, 'icon':...\n",
       "1     [{'measure': None, 'weightCatalog': 3, 'icon':...\n",
       "2     [{'measure': None, 'weightCatalog': 3, 'icon':...\n",
       "3     [{'measure': 'km', 'weightCatalog': 1, 'icon':...\n",
       "4     [{'measure': None, 'weightCatalog': 2, 'icon':...\n",
       "5     [{'measure': 'km', 'weightCatalog': 1, 'icon':...\n",
       "6     [{'measure': 'km', 'weightCatalog': 1, 'icon':...\n",
       "7     [{'measure': None, 'weightCatalog': 2, 'icon':...\n",
       "8     [{'measure': None, 'weightCatalog': 2, 'icon':...\n",
       "9     [{'measure': None, 'weightCatalog': 3, 'icon':...\n",
       "10    [{'measure': 'km', 'weightCatalog': 1, 'icon':...\n",
       "11    [{'measure': 'km', 'weightCatalog': 1, 'icon':...\n",
       "13    [{'measure': None, 'weightCatalog': 3, 'icon':...\n",
       "14    [{'measure': None, 'weightCatalog': 3, 'icon':...\n",
       "15    [{'measure': None, 'weightCatalog': 3, 'icon':...\n",
       "16    [{'measure': 'km', 'weightCatalog': 1, 'icon':...\n",
       "17    [{'measure': None, 'weightCatalog': 2, 'icon':...\n",
       "18    [{'measure': None, 'weightCatalog': 3, 'icon':...\n",
       "19    [{'measure': None, 'weightCatalog': 3, 'icon':...\n",
       "20    [{'measure': 'km', 'weightCatalog': 1, 'icon':...\n",
       "21    [{'measure': None, 'weightCatalog': 2, 'icon':...\n",
       "22    [{'measure': None, 'weightCatalog': 2, 'icon':...\n",
       "23    [{'measure': 'km', 'weightCatalog': 1, 'icon':...\n",
       "24    [{'measure': 'km', 'weightCatalog': 1, 'icon':...\n",
       "25    [{'measure': None, 'weightCatalog': 3, 'icon':...\n",
       "27    [{'measure': 'km', 'weightCatalog': 1, 'icon':...\n",
       "28    [{'measure': None, 'weightCatalog': 2, 'icon':...\n",
       "29    [{'measure': None, 'weightCatalog': 2, 'icon':...\n",
       "30    [{'measure': None, 'weightCatalog': 2, 'icon':...\n",
       "31    [{'measure': None, 'weightCatalog': 3, 'icon':...\n",
       "32    [{'measure': None, 'weightCatalog': 2, 'icon':...\n",
       "33    [{'measure': None, 'weightCatalog': 2, 'icon':...\n",
       "34    [{'measure': 'km', 'weightCatalog': 1, 'icon':...\n",
       "35    [{'measure': None, 'weightCatalog': 2, 'icon':...\n",
       "36    [{'measure': 'km', 'weightCatalog': 1, 'icon':...\n",
       "Name: specifications, dtype: object"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['specifications'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9972 entries, 0 to 9971\n",
      "Data columns (total 39 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   code                       9972 non-null   object        \n",
      " 1   city                       9972 non-null   object        \n",
      " 2   media                      9971 non-null   object        \n",
      " 3   specifications             9971 non-null   object        \n",
      " 4   createdAt                  9972 non-null   datetime64[ns]\n",
      " 5   price                      9972 non-null   float64       \n",
      " 6   listingStatus              971 non-null    object        \n",
      " 7   listingExpiration          964 non-null    datetime64[ns]\n",
      " 8   id                         9972 non-null   float64       \n",
      " 9   seo                        9972 non-null   object        \n",
      " 10  listing                    9972 non-null   object        \n",
      " 11  brand                      9971 non-null   object        \n",
      " 12  product                    9971 non-null   object        \n",
      " 13  refreshed                  4676 non-null   object        \n",
      " 14  oldPrice                   1043 non-null   float64       \n",
      " 15  categoryTitle              9972 non-null   object        \n",
      " 16  priceOnRequest             9972 non-null   object        \n",
      " 17  priceType                  9972 non-null   float64       \n",
      " 18  listingActivated           964 non-null    datetime64[ns]\n",
      " 19  category                   9972 non-null   float64       \n",
      " 20  user                       9972 non-null   float64       \n",
      " 21  status                     9972 non-null   object        \n",
      " 22  titleCompiled              9972 non-null   object        \n",
      " 23  activated                  9972 non-null   datetime64[ns]\n",
      " 24  ad_link                    9972 non-null   object        \n",
      " 25  country                    9972 non-null   int64         \n",
      " 26  region                     9972 non-null   int64         \n",
      " 27  city_id                    9972 non-null   int64         \n",
      " 28  brand_id                   9971 non-null   float64       \n",
      " 29  product_id                 9971 non-null   float64       \n",
      " 30  series                     3350 non-null   object        \n",
      " 31  model                      9971 non-null   object        \n",
      " 32  fuel                       9968 non-null   object        \n",
      " 33  km                         9970 non-null   object        \n",
      " 34  year                       9965 non-null   object        \n",
      " 35  compatibilityBrands        1 non-null      object        \n",
      " 36  compatibilitiesCount       1 non-null      float64       \n",
      " 37  compatibilityBrandsCount   1 non-null      float64       \n",
      " 38  compatibilityProductCount  1 non-null      float64       \n",
      "dtypes: datetime64[ns](4), float64(11), int64(3), object(21)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "ads.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ads.drop(ads.columns[-4:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['code', 'city', 'media', 'specifications', 'createdAt', 'price',\n",
       "       'listingStatus', 'listingExpiration', 'id', 'seo', 'listing', 'brand',\n",
       "       'product', 'refreshed', 'oldPrice', 'categoryTitle', 'priceOnRequest',\n",
       "       'priceType', 'listingActivated', 'category', 'user', 'status',\n",
       "       'titleCompiled', 'activated', 'ad_link', 'country', 'region', 'city_id',\n",
       "       'brand_id', 'product_id', 'series', 'model', 'fuel', 'km', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createdAt</th>\n",
       "      <th>listingActivated</th>\n",
       "      <th>activated</th>\n",
       "      <th>listingExpiration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-28 08:29:25</td>\n",
       "      <td>2021-05-14 06:49:12</td>\n",
       "      <td>2021-05-15 13:48:23</td>\n",
       "      <td>2021-05-24 06:49:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-29 16:13:33</td>\n",
       "      <td>2021-05-14 06:49:51</td>\n",
       "      <td>2021-05-15 13:46:45</td>\n",
       "      <td>2021-05-24 06:49:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-05 20:36:38</td>\n",
       "      <td>2021-05-15 06:37:03</td>\n",
       "      <td>2021-05-15 13:08:28</td>\n",
       "      <td>2021-05-25 06:37:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-15 11:33:26</td>\n",
       "      <td>2021-05-15 12:30:59</td>\n",
       "      <td>2021-05-15 12:30:59</td>\n",
       "      <td>2021-05-25 12:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-15 12:24:04</td>\n",
       "      <td>2021-05-15 12:25:08</td>\n",
       "      <td>2021-05-15 12:26:02</td>\n",
       "      <td>2021-05-25 12:25:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9967</th>\n",
       "      <td>2020-08-15 10:16:57</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-12-14 20:11:57</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9968</th>\n",
       "      <td>2020-12-14 17:22:43</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-12-14 18:40:36</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9969</th>\n",
       "      <td>2020-12-14 17:43:40</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-12-14 18:39:33</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>2020-12-14 17:16:12</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-12-14 17:18:08</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>2020-08-10 20:34:56</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-12-14 16:31:48</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9972 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               createdAt    listingActivated           activated  \\\n",
       "0    2021-03-28 08:29:25 2021-05-14 06:49:12 2021-05-15 13:48:23   \n",
       "1    2021-03-29 16:13:33 2021-05-14 06:49:51 2021-05-15 13:46:45   \n",
       "2    2021-05-05 20:36:38 2021-05-15 06:37:03 2021-05-15 13:08:28   \n",
       "3    2021-05-15 11:33:26 2021-05-15 12:30:59 2021-05-15 12:30:59   \n",
       "4    2021-05-15 12:24:04 2021-05-15 12:25:08 2021-05-15 12:26:02   \n",
       "...                  ...                 ...                 ...   \n",
       "9967 2020-08-15 10:16:57                 NaT 2020-12-14 20:11:57   \n",
       "9968 2020-12-14 17:22:43                 NaT 2020-12-14 18:40:36   \n",
       "9969 2020-12-14 17:43:40                 NaT 2020-12-14 18:39:33   \n",
       "9970 2020-12-14 17:16:12                 NaT 2020-12-14 17:18:08   \n",
       "9971 2020-08-10 20:34:56                 NaT 2020-12-14 16:31:48   \n",
       "\n",
       "       listingExpiration  \n",
       "0    2021-05-24 06:49:12  \n",
       "1    2021-05-24 06:49:51  \n",
       "2    2021-05-25 06:37:03  \n",
       "3    2021-05-25 12:30:59  \n",
       "4    2021-05-25 12:25:08  \n",
       "...                  ...  \n",
       "9967                 NaT  \n",
       "9968                 NaT  \n",
       "9969                 NaT  \n",
       "9970                 NaT  \n",
       "9971                 NaT  \n",
       "\n",
       "[9972 rows x 4 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['createdAt', 'listingActivated', 'activated', 'listingExpiration']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
