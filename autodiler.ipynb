{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "pd.get_option(\"display.max_columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_tree(source, limit=0, n=0):\n",
    "    \"\"\"\n",
    "    Function prints the structure (a \"tree\") of json code. The depth can be limited by passing \"limit\". n=0 starts a counter from 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    if n == limit and limit:\n",
    "        return\n",
    "    for key in source:\n",
    "        print(\"    \" * n, n+1, \"-\", key)\n",
    "        if isinstance(source[key], dict):\n",
    "            print_tree(source[key], limit, n+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(pages=0):\n",
    "    \"\"\"\n",
    "    Function parses autodiler.me website. As a result we obtain pandas dataframe with all the advertisements.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = 'https://www.autodiler.me/automobili/all/'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0'}\n",
    "\n",
    "    n = int()\n",
    "    while True:\n",
    "        n += 1\n",
    "        print(f\"Parsing page number  {n}\")\n",
    "        r = requests.get(url + str(n), headers=headers)\n",
    "        if r.status_code != 200:\n",
    "            print(f'status_code: {r.status_code}   >>>   page number: {num}')\n",
    "\n",
    "        # optionally we can save the response to a file\n",
    "#         with open(f'/home/pavlk/Documents/python-projects/autodiler/pages/page-{num}.txt', 'wb') as f:\n",
    "#             f.write(r.content)\n",
    "\n",
    "        # and then work with every file individually\n",
    "#         files_dir = os.path.join(os.getcwd(), 'pages')\n",
    "#         files = os.scandir(files_dir)\n",
    "#         for file in files:\n",
    "#             code = BeautifulSoup(open(os.path.join(files_dir, file), 'rb'), 'html.parser')\n",
    "\n",
    "        # pass response to Beautiful Soup for parsing\n",
    "        code = BeautifulSoup(r.content, 'html.parser')\n",
    "        # find all script tags and take the second one\n",
    "        script_tag = code.find_all('script')[1].string\n",
    "        # strip all the unnecessary information in that tag\n",
    "        json_string = script_tag[\n",
    "            script_tag.find('{'):\n",
    "            script_tag.find('module={')-1\n",
    "            ].strip()\n",
    "        # load parsed json string\n",
    "        src = json.loads(json_string)\n",
    "\n",
    "        # if necessary\n",
    "#         print_tree(src)\n",
    "\n",
    "        data = pd.DataFrame(src['props']['initialState']['ads']['list']['elements'])\n",
    "\n",
    "        # exit the cycle if nothing was obtained\n",
    "        if data.shape[0] == 0 and n == 1:\n",
    "            print('::ERROR::Something has changed...')\n",
    "            return None\n",
    "        elif data.shape[0] == 0:\n",
    "            print(f'{n}  pages were parsed')\n",
    "            return ads\n",
    "        \n",
    "        \n",
    "        \n",
    "        # MOVING TO PANDAS AND DATA PROCESSING\n",
    "        \n",
    "        # kick out the first wierd row and column\n",
    "        data.dropna(subset=['code'], inplace=True)\n",
    "        # drop useless columns\n",
    "        data.drop(labels=['banner', 'labelDescription', 'compatibilities'], axis=1, inplace=True)\n",
    "        # look foor empty columns...\n",
    "        empty = data.count().loc[data.count()==0].index\n",
    "        # ...and drop them\n",
    "        data = data.drop(labels=empty, axis=1)\n",
    "        \n",
    "        # get ad image\n",
    "        data['media'] = data['media'].apply(lambda x: x[0]['fd'])\n",
    "        \n",
    "        # function for building publication link\n",
    "        def generate_adlink(row):\n",
    "            return 'https://www.autodiler.me/' + row['categoryTitle'] + '/' + row['seo']\n",
    "        \n",
    "        # create publications links\n",
    "        data['ad_link'] = data.apply(generate_adlink, axis=1)\n",
    "        \n",
    "        # get ad location and location id\n",
    "        data['country'] = data['city'].apply(lambda x: x['country'])\n",
    "        data['region'] = data['city'].apply(lambda x: x['region'])\n",
    "        data['city_id'] = data['city'].apply(lambda x: x['id'])\n",
    "        data['city'] = data['city'].apply(lambda x: x['name'])\n",
    "        \n",
    "        # get vehicle make and its id\n",
    "        data['brand_id'] = data['brand'].apply(lambda x: x['id'])\n",
    "        data['brand'] = data['brand'].apply(lambda x: x['name'])\n",
    "        \n",
    "        # get vehicle model, series and model id\n",
    "        data['product_id'] = data['product'].apply(lambda x: x['id'])\n",
    "        data['series'] = data['product'].apply(lambda x: x['series'])\n",
    "        data['model'] = data['product'].apply(lambda x: x['model'])\n",
    "        data = data.drop('product', axis=1)\n",
    "        \n",
    "        # drop thumbnail column\n",
    "        data = data.drop('thumbnail', axis=1)\n",
    "\n",
    "        # process a set of dates\n",
    "        for date_col in ['createdAt', 'listingExpiration', 'listingActivated', 'activated']:\n",
    "            # !sometimes columns \"listingExpiration\" and \"listingActivated\" are excluded\n",
    "            if date_col in data.columns:\n",
    "                data[date_col] = pd.to_datetime(data[date_col], format='%Y-%m-%dT%H:%M:%S.000Z')\n",
    "\n",
    "        # functions to get fuel type, milage and year\n",
    "        def get_fuel(spec_list):\n",
    "            for spec in spec_list:\n",
    "                if str(spec['title']).lower().startswith('gorivo'):\n",
    "                    return spec['value']\n",
    "\n",
    "        def get_year(spec_list):\n",
    "            for spec in spec_list:\n",
    "                if str(spec['title']).lower().startswith('godi'):\n",
    "                    return spec['value']\n",
    "\n",
    "        def get_km(spec_list):\n",
    "            for spec in spec_list:\n",
    "                if str(spec['title']).lower().startswith('kilometra'):\n",
    "                    return spec['value']\n",
    "                \n",
    "        # apply above metioned functions\n",
    "        data['fuel'] = data['specifications'].apply(get_fuel)\n",
    "        data['km'] = data['specifications'].apply(get_km)\n",
    "        data['year'] = data['specifications'].apply(get_year)\n",
    "        \n",
    "        # appending dataframes\n",
    "        if n == 1:\n",
    "            ads = data.copy()\n",
    "        else:\n",
    "            ads = ads.append(data, ignore_index=True)\n",
    "        \n",
    "        # break if page limit was set\n",
    "        if n == pages:\n",
    "            print(f'{n}  pages were parsed')\n",
    "            return ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing page number  1\n",
      "Parsing page number  2\n",
      "Parsing page number  3\n",
      "Parsing page number  4\n",
      "Parsing page number  5\n",
      "Parsing page number  6\n",
      "Parsing page number  7\n",
      "Parsing page number  8\n",
      "Parsing page number  9\n",
      "Parsing page number  10\n",
      "Parsing page number  11\n",
      "Parsing page number  12\n",
      "Parsing page number  13\n",
      "Parsing page number  14\n",
      "Parsing page number  15\n",
      "Parsing page number  16\n",
      "Parsing page number  17\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-1beeb75ff477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-126-b1000c3038e7>\u001b[0m in \u001b[0;36mparsing\u001b[0;34m(pages)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# get ad image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'media'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'media'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# function for building publication link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/APPS/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4200\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-b1000c3038e7>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# get ad image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'media'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'media'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# function for building publication link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ads = parsing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108 entries, 0 to 107\n",
      "Data columns (total 34 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   code               108 non-null    object        \n",
      " 1   city               108 non-null    object        \n",
      " 2   media              108 non-null    object        \n",
      " 3   specifications     108 non-null    object        \n",
      " 4   createdAt          108 non-null    datetime64[ns]\n",
      " 5   price              108 non-null    float64       \n",
      " 6   listingStatus      108 non-null    object        \n",
      " 7   listingExpiration  108 non-null    datetime64[ns]\n",
      " 8   id                 108 non-null    float64       \n",
      " 9   seo                108 non-null    object        \n",
      " 10  listing            108 non-null    object        \n",
      " 11  brand              108 non-null    object        \n",
      " 12  refreshed          89 non-null     object        \n",
      " 13  oldPrice           16 non-null     float64       \n",
      " 14  categoryTitle      108 non-null    object        \n",
      " 15  priceOnRequest     108 non-null    object        \n",
      " 16  priceType          108 non-null    float64       \n",
      " 17  listingActivated   108 non-null    datetime64[ns]\n",
      " 18  category           108 non-null    float64       \n",
      " 19  user               108 non-null    float64       \n",
      " 20  status             108 non-null    object        \n",
      " 21  titleCompiled      108 non-null    object        \n",
      " 22  activated          108 non-null    datetime64[ns]\n",
      " 23  ad_link            108 non-null    object        \n",
      " 24  country            108 non-null    int64         \n",
      " 25  region             108 non-null    int64         \n",
      " 26  city_id            108 non-null    int64         \n",
      " 27  brand_id           108 non-null    int64         \n",
      " 28  product_id         108 non-null    int64         \n",
      " 29  series             33 non-null     object        \n",
      " 30  model              108 non-null    object        \n",
      " 31  fuel               108 non-null    object        \n",
      " 32  km                 108 non-null    object        \n",
      " 33  year               108 non-null    object        \n",
      "dtypes: datetime64[ns](4), float64(6), int64(5), object(19)\n",
      "memory usage: 28.8+ KB\n"
     ]
    }
   ],
   "source": [
    "ads.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.autodiler.me/automobili/all/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0'}\n",
    "\n",
    "# set page number\n",
    "n = int()\n",
    "while True:\n",
    "    print(f\"Parsing page number  {n}\")\n",
    "    r = requests.get(url + str(n), headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        print(f'status_code: {r.status_code}   >>>   page number: {num}')\n",
    "\n",
    "    # optionally we can save the response to a file\n",
    "#         with open(f'/home/pavlk/Documents/python-projects/autodiler/pages/page-{num}.txt', 'wb') as f:\n",
    "#             f.write(r.content)\n",
    "\n",
    "    # and then work with every file individually\n",
    "#         files_dir = os.path.join(os.getcwd(), 'pages')\n",
    "#         files = os.scandir(files_dir)\n",
    "#         for file in files:\n",
    "#             code = BeautifulSoup(open(os.path.join(files_dir, file), 'rb'), 'html.parser')\n",
    "\n",
    "    # pass response to Beautiful Soup for parsing\n",
    "    code = BeautifulSoup(r.content, 'html.parser')\n",
    "    # find all script tags and take the second one\n",
    "    script_tag = code.find_all('script')[1].string\n",
    "    # strip all the unnecessary information in that tag\n",
    "    json_string = script_tag[\n",
    "        script_tag.find('{'):\n",
    "        script_tag.find('module={')-1\n",
    "        ].strip()\n",
    "    # load parsed json string\n",
    "    src = json.loads(json_string)\n",
    "\n",
    "    # if necessary\n",
    "#         print_tree(src)\n",
    "\n",
    "    data = pd.DataFrame(src['props']['initialState']['ads']['list']['elements'])\n",
    "\n",
    "    # exit the cycle if nothing was obtained\n",
    "    if data.shape[0] == 0 and n == 1:\n",
    "        print('::ERROR::Something has changed...')\n",
    "        return None\n",
    "    elif data.shape[0] == 0:\n",
    "        print(f'{n}  pages were parsed')\n",
    "        return ads\n",
    "\n",
    "\n",
    "\n",
    "    # MOVING TO PANDAS AND DATA PROCESSING\n",
    "\n",
    "    # kick out the first wierd row and column\n",
    "    data.dropna(subset=['code'], inplace=True)\n",
    "    # drop useless columns\n",
    "    data.drop(labels=['banner', 'labelDescription', 'compatibilities'], axis=1, inplace=True)\n",
    "    # look foor empty columns...\n",
    "    empty = data.count().loc[data.count()==0].index\n",
    "    # ...and drop them\n",
    "    data = data.drop(labels=empty, axis=1)\n",
    "\n",
    "    # get ad image\n",
    "    data['media'] = data['media'].apply(lambda x: x[0]['fd'])\n",
    "\n",
    "    # function for building publication link\n",
    "    def generate_adlink(row):\n",
    "        return 'https://www.autodiler.me/' + row['categoryTitle'] + '/' + row['seo']\n",
    "\n",
    "    # create publications links\n",
    "    data['ad_link'] = data.apply(generate_adlink, axis=1)\n",
    "\n",
    "    # get ad location and location id\n",
    "    data['country'] = data['city'].apply(lambda x: x['country'])\n",
    "    data['region'] = data['city'].apply(lambda x: x['region'])\n",
    "    data['city_id'] = data['city'].apply(lambda x: x['id'])\n",
    "    data['city'] = data['city'].apply(lambda x: x['name'])\n",
    "\n",
    "    # get vehicle make and its id\n",
    "    data['brand_id'] = data['brand'].apply(lambda x: x['id'])\n",
    "    data['brand'] = data['brand'].apply(lambda x: x['name'])\n",
    "\n",
    "    # get vehicle model, series and model id\n",
    "    data['product_id'] = data['product'].apply(lambda x: x['id'])\n",
    "    data['series'] = data['product'].apply(lambda x: x['series'])\n",
    "    data['model'] = data['product'].apply(lambda x: x['model'])\n",
    "    data = data.drop('product', axis=1)\n",
    "\n",
    "    # drop thumbnail column\n",
    "    data = data.drop('thumbnail', axis=1)\n",
    "\n",
    "    # process a set of dates\n",
    "    for date_col in ['createdAt', 'listingExpiration', 'listingActivated', 'activated']:\n",
    "        # !sometimes columns \"listingExpiration\" and \"listingActivated\" are excluded\n",
    "        if date_col in data.columns:\n",
    "            data[date_col] = pd.to_datetime(data[date_col], format='%Y-%m-%dT%H:%M:%S.000Z')\n",
    "\n",
    "    # functions to get fuel type, milage and year\n",
    "    def get_fuel(spec_list):\n",
    "        for spec in spec_list:\n",
    "            if str(spec['title']).lower().startswith('gorivo'):\n",
    "                return spec['value']\n",
    "\n",
    "    def get_year(spec_list):\n",
    "        for spec in spec_list:\n",
    "            if str(spec['title']).lower().startswith('godi'):\n",
    "                return spec['value']\n",
    "\n",
    "    def get_km(spec_list):\n",
    "        for spec in spec_list:\n",
    "            if str(spec['title']).lower().startswith('kilometra'):\n",
    "                return spec['value']\n",
    "\n",
    "    # apply above metioned functions\n",
    "    data['fuel'] = data['specifications'].apply(get_fuel)\n",
    "    data['km'] = data['specifications'].apply(get_km)\n",
    "    data['year'] = data['specifications'].apply(get_year)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
